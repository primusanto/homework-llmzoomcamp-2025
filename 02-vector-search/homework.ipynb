{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc65a50",
   "metadata": {},
   "source": [
    "Q1. Embedding the query\n",
    "Embed the query: 'I just discovered the course. Can I join now?'. Use the 'jinaai/jina-embeddings-v2-small-en' model.\n",
    "\n",
    "You should get a numpy array of size 512.\n",
    "\n",
    "What's the minimal value in this array?\n",
    "\n",
    "-0.51\n",
    "-0.11\n",
    "0\n",
    "0.51\n",
    "Cosine similarity\n",
    "The vectors that our embedding model returns are already normalized: their length is 1.0.\n",
    "\n",
    "You can chech that by using the norm function:\n",
    "\n",
    "import numpy as np\n",
    "np.linalg.norm(q)\n",
    "Which means that we can simply compute the dot product between two vectors to learn the cosine similarity between them.\n",
    "\n",
    "For example, if you compute the cosine of the query vector with itself, the result will be 1.0:\n",
    "\n",
    "q.dot(q)m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04331a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7cf52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastembed import TextEmbedding\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce221731",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "query = 'I just discovered the course. Can I join now?' \n",
    "embedding_dimensiality = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e3a67c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TextEmbedding(model_name=model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e290e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jinaai/jina-embeddings-v2-small-en'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "embedding_model.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "589c5cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the query vector: 512\n",
      "the minimum value in the query vector: -0.11726373551188797\n",
      "the norm of the query vector: 1.0\n",
      "the dot product of the query vector with itself: 1.0\n"
     ]
    }
   ],
   "source": [
    "q_list = list(embedding_model.embed(query))\n",
    "q = q_list[0] # Get the first query\n",
    "#print the length of the query vector\n",
    "print(f'length of the query vector: {len(q)}')\n",
    "#print the minimum value in the query vector\n",
    "print(f'the minimum value in the query vector: {min(q)}')\n",
    "\n",
    "#print the norm of the query vector\n",
    "print(f'the norm of the query vector: {np.linalg.norm(q)}')\n",
    "#print the dot product of the query vector with itself\n",
    "print(f'the dot product of the query vector with itself: {q.dot(q)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105a926d",
   "metadata": {},
   "source": [
    "Q2. Cosine similarity with another vector\n",
    "Now let's embed this document:\n",
    "\n",
    "doc = 'Can I still join the course after the start date?'\n",
    "What's the cosine similarity between the vector for the query and the vector for the document?\n",
    "\n",
    "0.3\n",
    "0.5\n",
    "0.7\n",
    "0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d00e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 'Can I still join the course after the start date?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded9458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the doc vector: 512\n",
      "the minimum value in the doc vector: -0.12396319741919225\n",
      "the norm of the doc vector: 0.9999999999999999\n",
      "the dot product of the query vector with the doc vector: 0.9008528856818037\n"
     ]
    }
   ],
   "source": [
    "doc_list = list(embedding_model.embed(doc))\n",
    "d = doc_list[0] # Get the first doc\n",
    "#print the length of the doc vector\n",
    "print(f'length of the doc vector: {len(d)}')\n",
    "#print the minimum value in the doc vector\n",
    "print(f'the minimum value in the doc vector: {min(d)}')\n",
    "\n",
    "#print the norm of the doc vector\n",
    "print(f'the norm of the doc vector: {np.linalg.norm(d)}')\n",
    "#print the vector similarity between the query and the doc\n",
    "print(f'the dot product of the query vector with the doc vector: {q.dot(d)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16fbe64",
   "metadata": {},
   "source": [
    "Q3. Ranking by cosine\n",
    "\n",
    "Compute the embeddings for the text field, and compute the cosine between the query vector and all the documents.\n",
    "\n",
    "What's the document index with the highest similarity? (Indexing starts from 0):\n",
    "\n",
    "0\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "Hint: if you put all the embeddings of the text field in one matrix V (a single 2-dimensional numpy array), then computing the cosine becomes a matrix multiplication:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c0d522cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [{'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
    "  'section': 'General course-related questions',\n",
    "  'question': 'Course - Can I still join the course after the start date?',\n",
    "  'course': 'data-engineering-zoomcamp'},\n",
    " {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
    "  'section': 'General course-related questions',\n",
    "  'question': 'Course - Can I follow the course after it finishes?',\n",
    "  'course': 'data-engineering-zoomcamp'},\n",
    " {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
    "  'section': 'General course-related questions',\n",
    "  'question': 'Course - When will the course start?',\n",
    "  'course': 'data-engineering-zoomcamp'},\n",
    " {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
    "  'section': 'General course-related questions',\n",
    "  'question': 'Course - What can I do before the course starts?',\n",
    "  'course': 'data-engineering-zoomcamp'},\n",
    " {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.',\n",
    "  'section': 'General course-related questions',\n",
    "  'question': 'How can we contribute to the course?',\n",
    "  'course': 'data-engineering-zoomcamp'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9855d01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0:  dot product of the document vector with query: 0.7629684493123693\n",
      "Index 1:  dot product of the document vector with query: 0.8182378361919107\n",
      "Index 2:  dot product of the document vector with query: 0.8085397290762828\n",
      "Index 3:  dot product of the document vector with query: 0.7133078539597724\n",
      "Index 4:  dot product of the document vector with query: 0.7304499528359614\n",
      "Highest index: 1, dot product value: 0.8182378361919107\n"
     ]
    }
   ],
   "source": [
    "V_Q3 = list(embedding_model.embed(list(map(lambda doc : doc['text'], documents))))\n",
    "\n",
    "highest = 0\n",
    "for i, v_q3 in enumerate(V_Q3):\n",
    "    if v_q3.dot(q) > highest:\n",
    "        highest = v_q3.dot(q)\n",
    "        index = i\n",
    "    print(f'Index {i}:  dot product of the document vector with query: {v_q3.dot(q)}')\n",
    "\n",
    "#print highest index and the dot product value\n",
    "print(f'Highest index: {index}, dot product value: {highest}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4886330",
   "metadata": {},
   "source": [
    "Q4. Ranking by cosine, version two\n",
    "\n",
    "\n",
    "Now let's calculate a new field, which is a concatenation of question and text:\n",
    "\n",
    "full_text = doc['question'] + ' ' + doc['text']\n",
    "Embed this field and compute the cosine between it and the query vector. What's the highest scoring document?\n",
    "\n",
    "0\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "Is it different from Q3? If yes, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4521fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text =list(map(lambda doc : doc['question'] + ' ' + doc['text'], documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e57a7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0:  dot product of the document vector with query: 0.8514542983476503\n",
      "Index 1:  dot product of the document vector with query: 0.8436594058703555\n",
      "Index 2:  dot product of the document vector with query: 0.8408287206920106\n",
      "Index 3:  dot product of the document vector with query: 0.7755157689596764\n",
      "Index 4:  dot product of the document vector with query: 0.8086007942697778\n",
      "Highest index: 0, dot product value: 0.8514542983476503\n"
     ]
    }
   ],
   "source": [
    "V_Q4 = list(embedding_model.embed(list(map(lambda doc : doc['question'] + ' ' + doc['text'], documents))))\n",
    "\n",
    "highest = 0\n",
    "for i, v_q4 in enumerate(V_Q4):\n",
    "    if v_q4.dot(q) > highest:\n",
    "        highest = v_q4.dot(q)\n",
    "        index = i\n",
    "    print(f'Index {i}:  dot product of the document vector with query: {v_q4.dot(q)}')\n",
    "\n",
    "#print highest index and the dot product value\n",
    "print(f'Highest index: {index}, dot product value: {highest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d9cbb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Course - Can I still join the course after the start date?\n",
      "Text: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      "\n",
      "\n",
      "Question: Course - Can I follow the course after it finishes?\n",
      "Text: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "\n",
      "Question: Course - When will the course start?\n",
      "Text: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "\n",
      "Question: Course - What can I do before the course starts?\n",
      "Text: You can start by installing and setting up all the dependencies and requirements:\n",
      "Google cloud account\n",
      "Google Cloud SDK\n",
      "Python 3 (installed with Anaconda)\n",
      "Terraform\n",
      "Git\n",
      "Look over the prerequisites and syllabus to see if you are comfortable with these subjects.\n",
      "\n",
      "\n",
      "Question: How can we contribute to the course?\n",
      "Text: Star the repo! Share it with friends if you find it useful ❣️\n",
      "Create a PR if you see you can improve the text or the structure of the repository.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(f\"Question: {doc['question']}\")\n",
    "    print(f\"Text: {doc['text']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05ab798",
   "metadata": {},
   "source": [
    "WHY: Because the query has very similar wording to the question in the documents, the model is able to find the most relevant document based on the question text alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea82a66",
   "metadata": {},
   "source": [
    "Q5. Selecting the embedding model\n",
    "\n",
    "Now let's select a smaller embedding model. What's the smallest dimensionality for models in fastembed?\n",
    "\n",
    "128\n",
    "256\n",
    "384\n",
    "512\n",
    "One of these models is BAAI/bge-small-en. Let's use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3fe9b0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest dimensiality model: BAAI/bge-small-en with dimension 384\n"
     ]
    }
   ],
   "source": [
    "smallest_model = min(TextEmbedding.list_supported_models(), key=lambda model: model['dim'])\n",
    "print (f\"Smallest dimensiality model: {smallest_model['model']} with dimension {smallest_model['dim']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbfd3a",
   "metadata": {},
   "source": [
    "Q6. Indexing with qdrant (2 points)\n",
    "\n",
    "For the last question, we will use more documents.\n",
    "\n",
    "We will select only FAQ records from our ml zoomcamp:\n",
    "\n",
    "Add them to qdrant using the model form Q5.\n",
    "\n",
    "When adding the data, use both question and answer fields:\n",
    "\n",
    "What's the highest score in the results? (The score for the first returned record):\n",
    "\n",
    "0.97\n",
    "0.87\n",
    "0.77\n",
    "0.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b3725943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "\n",
    "documents_q6 = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "    if course_name != 'machine-learning-zoomcamp':\n",
    "        continue\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents_q6.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -q \"qdrant-client[fastembed]>=1.14.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3774bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bc06face",
   "metadata": {},
   "outputs": [],
   "source": [
    "qd_client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0fed0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSIONALITY = 384\n",
    "model_handle_q6 = \"BAAI/bge-small-en\"\n",
    "collection_name = \"zoomcamp-faq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1eea86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.delete_collection(collection_name=collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b99cb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=EMBEDDING_DIMENSIONALITY,\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e89ea3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "\n",
    "for i, doc in enumerate(documents_q6):\n",
    "    text = doc['question'] + ' ' + doc['text']\n",
    "    vector = models.Document(text=text, model=model_handle_q6)\n",
    "    point = models.PointStruct(\n",
    "        id=i,\n",
    "        vector=vector,\n",
    "        payload=doc\n",
    "    )\n",
    "    points.append(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d781620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:22<00:00,  4.43s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2f82a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(question):\n",
    "    print('vector_search is used')\n",
    "    \n",
    "    query_points = qd_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=question,\n",
    "            model=model_handle_q6 \n",
    "        ),\n",
    "        limit=5,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for point in query_points.points:\n",
    "        results.append(point)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a24e34e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_search is used\n",
      "Highest score: 0.8703172\n"
     ]
    }
   ],
   "source": [
    "results = vector_search(query)\n",
    "#print the higshest score and the question from the results\n",
    "print(f'Highest score: {results[0].score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
